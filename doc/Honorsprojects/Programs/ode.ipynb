{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Using Neural networks to solve Ordinary Differential Equations (ODEs) -->\n",
    "# Using Neural networks to solve Ordinary Differential Equations (ODEs)\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen, MSU -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen, MSU**\n",
    "\n",
    "Date: **Feb 26, 2020**\n",
    "\n",
    "## Solving eigenvalue problems as ODEs with Neural Networks\n",
    "\n",
    "The eigenvalues and eigenvectors of a matrix A is defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{A}\\boldsymbol{v} = \\lambda\\boldsymbol{v},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda$ is the eigenvalue and $\\boldsymbol{v}$,\n",
    "is the eigenvector. The problem of finding eigenvalues is important\n",
    "and many physical problems can be transformed to an eigenvalue problem.\n",
    "\n",
    "Here we use neural networks and **tensorflow** for solving an eigenvalue problem rewritten as\n",
    "an ordinary differential equation.\n",
    "\n",
    "The eigenvalue problem\n",
    "is transformed into a differential equation. The equation we will use to solve the problem is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dx(t)}{dt}= −x(t) + f(x(t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f$ is a function that contains the matrix $\\boldsymbol{A}$, defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(\\boldsymbol{x}) = [\\boldsymbol{x}^T \\boldsymbol{x}\\boldsymbol{A} + (\\boldsymbol{1} − \\boldsymbol{x}^T\\boldsymbol{A}\\boldsymbol{x})]\\boldsymbol{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $\\boldsymbol{A}$ is a symmetric $n\\times n$ matrix.\n",
    "\n",
    "This is a non-linear differential equation. The quantity $\\boldsymbol{x}$ is\n",
    "one of eigenvectors with entries $\\boldsymbol{x}=[x_1, x_2,\\dots,x_n]^T$ and it is an output\n",
    "from the neural network.\n",
    "\n",
    "It can be shown that any stationary point of the equation\n",
    "is an eigenvector of the matrix $\\boldsymbol{A}$.\n",
    "We can therefore disregard the time-dependency in $\\boldsymbol{x}$. The\n",
    "stationary state is reached when the derivative of $\\boldsymbol{x}$ wrt time is zero.\n",
    "This means that the problem is now simplified to solve\n",
    "$\\boldsymbol{x}(t)=f(\\boldsymbol{x}(t))$.\n",
    "\n",
    "\n",
    "The cost-function can then be set up as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{x}(t)) = \\mathrm{MSE}[\\boldsymbol{x}(t)−f(\\boldsymbol{x}(t))]=0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\boldsymbol{x}$ is the output of the network given some input. When the neural network trains, it will try\n",
    "to minimize the difference between $\\boldsymbol{x}(t)$ and $f(\\boldsymbol{x}(t))$ by\n",
    "tweaking the weights and biases. Starting\n",
    "from any non-zero input vector, the network will converge to the eigenvector corresponding to the\n",
    "largest eigenvalue.\n",
    "\n",
    "We can also make the network converge to other eigenvectors not corresponding to the largest\n",
    "eigenvalue. The easiest one to obtain is the one corresponding to the smallest eigenvalue. By using\n",
    "$-\\boldsymbol{A}$ instead of $\\boldsymbol{A}$ in $\\boldsymbol{f}$,\n",
    "we will converge to the eigenvector corresponding to the smallest eigenvalue\n",
    "of $\\boldsymbol{A}$.\n",
    "It is also possible to find the eigenvectors for the remaining eigenvalues. It can be shown\n",
    "that if the input vector, lets call it $\\boldsymbol{x}_0$, is orthogonal to the a set of $k$ eigenvectors, then the solution\n",
    "converges to an eigenvector that is orthogonal to the $k$ eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# # Finding eigenvalues of matrices with neural networks. \n",
    "# Script for finding the eigenvectors corresponding to the largest eigenvalue of a matrix with a neural network.\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "# tf.set_random_seed(343)\n",
    "\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "#from lib import compute_dx_dt\n",
    "\n",
    "matrix_size = 6\n",
    "\n",
    "A = np.random.random_sample(size=(matrix_size,matrix_size))\n",
    "A = (A.T + A)/2.0\n",
    "start_matrix = A\n",
    "\n",
    "eigen_vals, eigen_vecs =  np.linalg.eig(A)\n",
    "\n",
    "A = tf.convert_to_tensor(A)\n",
    "print(\"A = \", A)\n",
    "\n",
    "x_0 = tf.convert_to_tensor(np.random.random_sample(size = (1,matrix_size)))\n",
    "print(\"x0 = \", x_0)\n",
    "\n",
    "## The construction phase\n",
    "\n",
    "num_iter = 10000\n",
    "num_hidden_neurons = [50]\n",
    "num_hidden_layers = np.size(num_hidden_neurons)\n",
    "\n",
    "\n",
    "with tf.variable_scope('dnn'):\n",
    "\n",
    "    previous_layer = x_0\n",
    "\n",
    "    for l in range(num_hidden_layers):\n",
    "        current_layer = tf.layers.dense(previous_layer, num_hidden_neurons[l],activation=tf.nn.sigmoid)\n",
    "        previous_layer = current_layer\n",
    "\n",
    "    dnn_output = tf.layers.dense(previous_layer, matrix_size)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    print(\"dnn_output = \", dnn_output)\n",
    "    \n",
    "    x_trial = tf.transpose(dnn_output)\n",
    "    print(\"x_trial = \", x_trial)\n",
    "    \n",
    "    temp1 = (tf.tensordot(tf.transpose(x_trial), x_trial, axes=1)*A)\n",
    "    temp2 = (1- tf.tensordot(tf.transpose(x_trial), tf.tensordot(A, x_trial, axes=1), axes=1))*np.eye(matrix_size)\n",
    "    func = tf.tensordot((temp1-temp2), x_trial, axes=1)\n",
    "    \n",
    "    print(temp1)\n",
    "    print(temp2)\n",
    "    print(func)\n",
    "    \n",
    "    func = tf.transpose(func)\n",
    "    x_trial = tf.transpose(x_trial)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(func, x_trial)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    traning_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "g_dnn = None\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iter):\n",
    "        sess.run(traning_op)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            l = loss.eval()\n",
    "            print(\"Step:\", i, \"/\",num_iter, \"loss: \", l)\n",
    "            losses.append(l)\n",
    "\n",
    "    x_dnn = x_trial.eval()\n",
    "x_dnn = x_dnn.T\n",
    "\n",
    "\n",
    "# ## Plotting loss over time\n",
    "\n",
    "plt.plot(losses[:5])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "print(\"Eigenvector NN = \\n\", (x_dnn/(x_dnn**2).sum()**0.5), \"\\n\")\n",
    "\n",
    "eigen_val_nn = x_dnn.T @ (start_matrix @ x_dnn) / (x_dnn.T @ x_dnn)\n",
    "\n",
    "print(\"Eigenvalue NN = \\n\", eigen_val_nn, \"\\n \\n\")\n",
    "print(\"Eigenvector analytic = \\n\", eigen_vecs)\n",
    "print(\"\\n\")\n",
    "print(\"Eigenvalues analytic = \\n\",eigen_vals)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}

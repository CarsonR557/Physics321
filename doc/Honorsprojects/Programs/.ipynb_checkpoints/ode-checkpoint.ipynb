{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Using Neural networks to solve Ordinary Differential Equations (ODEs) -->\n",
    "# Using Neural networks to solve Ordinary Differential Equations (ODEs)\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen, MSU -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen, MSU**\n",
    "\n",
    "Date: **Feb 26, 2020**\n",
    "\n",
    "## Example: Exponential decay\n",
    "An exponential decay of a quantity $g(x)$ is described by the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"solve_expdec\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{solve_expdec} \\tag{1}\n",
    "  g'(x) = -\\gamma g(x)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $g(0) = g_0$ for some chosen initial value $g_0$.\n",
    "\n",
    "The analytical solution of ([1](#solve_expdec)) is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  g(x) = g_0 \\exp\\left(-\\gamma x\\right)\n",
    "\\label{_auto1} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having an analytical solution at hand, it is possible to use it to compare how well a neural network finds a solution of ([1](#solve_expdec)).\n",
    "\n",
    "\n",
    "\n",
    "We will use a neural network to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"solveode\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{solveode} \\tag{3}\n",
    "g'(x) = -\\gamma g(x)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $g(0) = g_0$ with $\\gamma$ and $g_0$ being some chosen values.\n",
    "\n",
    "In this example, $\\gamma = 2$ and $g_0 = 10$.\n",
    "\n",
    "## The trial solution\n",
    "To begin with, a trial solution $g_t(t)$ must be chosen. A general trial solution for ordinary differential equations could be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g_t(x, P) = h_1(x) + h_2(x, N(x, P))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $h_1(x)$ ensuring that $g_t(x)$ satisfies some conditions and\n",
    "$h_2(x,N(x, P))$ an expression involving $x$ and the output from the\n",
    "neural network $N(x,P)$ with $P $ being the collection of the weights\n",
    "and biases for each layer. For now, it is assumed that the network\n",
    "consists of one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "\n",
    "\n",
    "## Reformulating the problem\n",
    "We wish that our neural network manages to minimize a given cost function.\n",
    "\n",
    "A reformulation of our equation must therefore be done,\n",
    "such that it describes the problem a neural network can solve for.\n",
    "\n",
    "The trial solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "g_t(x, P) = g_0 + x \\cdot N(x, P)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has been chosen such that it already solves the condition $g(0) = g_0$. What remains, is to find $P$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"nnmin\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{nnmin} \\tag{4}\n",
    "g_t'(x, P) = - \\gamma g_t(x, P)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is fulfilled as *best as possible*.\n",
    "\n",
    "The left hand side and right hand side of ([4](#nnmin)) must be computed separately, and then the neural network must choose weights and biases, contained in $P$, such that the sides are equal as best as possible.\n",
    "This means that the absolute or squared difference between the sides must be as close to zero, ideally equal to zero.\n",
    "In this case, the difference squared shows to be an appropriate measurement of how erroneous the trial solution is with respect to $P$ of the neural network.\n",
    "\n",
    "This gives the following cost function our neural network must solve for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{P}\\Big\\{ \\big(g_t'(x, P) - ( -\\gamma g_t(x, P) \\big)^2 \\Big\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the notation $\\min_{P}\\{ f(x, P) \\}$ means that we desire to find $P$ that yields the minimum of $f(x, P)$)\n",
    "\n",
    "or, in terms of weights and biases for the hidden and output layer in our network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{P_{\\text{hidden} }, \\ P_{\\text{output} }}\\Big\\{ \\big(g_t'(x, \\{ P_{\\text{hidden} }, P_{\\text{output} }\\}) - ( -\\gamma g_t(x, \\{ P_{\\text{hidden} }, P_{\\text{output} }\\}) \\big)^2 \\Big\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for an input value $x$.\n",
    "\n",
    "If the neural network evaluates $g_t(x, P)$ at more values for $x$, say $N$ values $x_i$ for $i = 1, \\dots, N$, then the *total* error to minimize becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"min\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\label{min} \\tag{5}\n",
    "\\min_{P}\\Big\\{\\frac{1}{N} \\sum_{i=1}^N  \\big(g_t'(x_i, P) - ( -\\gamma g_t(x_i, P) \\big)^2 \\Big\\}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $\\vec x$ be a vector with elements $x_i$ and $c(\\vec x, P) = \\frac{1}{N} \\sum_i  \\big(g_t'(x_i, P) - ( -\\gamma g_t(x_i, P) \\big)^2$ denote the cost function, the minimization problem that our network must solve, becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{P} c(\\vec x, P)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of $P_{\\text{hidden} }$ and $P_{\\text{output} }$, this could also be expressed as\n",
    "\n",
    "$$\n",
    "\\min_{P_{\\text{hidden} }, \\ P_{\\text{output} }} c(\\vec x, \\{P_{\\text{hidden} }, P_{\\text{output} }\\})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "A =  Tensor(\"Const:0\", shape=(6, 6), dtype=float64)\n",
      "x0 =  Tensor(\"Const_1:0\", shape=(1, 6), dtype=float64)\n",
      "WARNING:tensorflow:From <ipython-input-1-580cbdad9716>:44: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "dnn_output =  Tensor(\"dnn/dense_1/BiasAdd:0\", shape=(1, 6), dtype=float64)\n",
      "x_trial =  Tensor(\"loss/transpose:0\", shape=(6, 1), dtype=float64)\n",
      "Tensor(\"loss/mul:0\", shape=(6, 6), dtype=float64)\n",
      "Tensor(\"loss/mul_1:0\", shape=(6, 6), dtype=float64)\n",
      "Tensor(\"loss/Tensordot_3:0\", shape=(6, 1), dtype=float64)\n",
      "Step: 0 / 10000 loss:  0.8811536\n",
      "Step: 100 / 10000 loss:  0.046221524\n",
      "Step: 200 / 10000 loss:  0.016936405\n",
      "Step: 300 / 10000 loss:  0.0077625248\n",
      "Step: 400 / 10000 loss:  0.0039261067\n",
      "Step: 500 / 10000 loss:  0.002088301\n",
      "Step: 600 / 10000 loss:  0.0011415791\n",
      "Step: 700 / 10000 loss:  0.0006338091\n",
      "Step: 800 / 10000 loss:  0.00035512898\n",
      "Step: 900 / 10000 loss:  0.00020010369\n",
      "Step: 1000 / 10000 loss:  0.00011315974\n",
      "Step: 1100 / 10000 loss:  6.4148364e-05\n",
      "Step: 1200 / 10000 loss:  3.6427515e-05\n",
      "Step: 1300 / 10000 loss:  2.071231e-05\n",
      "Step: 1400 / 10000 loss:  1.178851e-05\n",
      "Step: 1500 / 10000 loss:  6.714881e-06\n",
      "Step: 1600 / 10000 loss:  3.827342e-06\n",
      "Step: 1700 / 10000 loss:  2.1827416e-06\n",
      "Step: 1800 / 10000 loss:  1.2453808e-06\n",
      "Step: 1900 / 10000 loss:  7.1084213e-07\n",
      "Step: 2000 / 10000 loss:  4.0587983e-07\n",
      "Step: 2100 / 10000 loss:  2.3182183e-07\n",
      "Step: 2200 / 10000 loss:  1.3244498e-07\n",
      "Step: 2300 / 10000 loss:  7.567783e-08\n",
      "Step: 2400 / 10000 loss:  4.325354e-08\n",
      "Step: 2500 / 10000 loss:  2.4727095e-08\n",
      "Step: 2600 / 10000 loss:  1.4136344e-08\n",
      "Step: 2700 / 10000 loss:  8.083139e-09\n",
      "Step: 2800 / 10000 loss:  4.6220157e-09\n",
      "Step: 2900 / 10000 loss:  2.6442566e-09\n",
      "Step: 3000 / 10000 loss:  1.5119698e-09\n",
      "Step: 3100 / 10000 loss:  8.6484375e-10\n",
      "Step: 3200 / 10000 loss:  4.94961e-10\n",
      "Step: 3300 / 10000 loss:  2.828026e-10\n",
      "Step: 3400 / 10000 loss:  1.6201303e-10\n",
      "Step: 3500 / 10000 loss:  9.2457965e-11\n",
      "Step: 3600 / 10000 loss:  5.295579e-11\n",
      "Step: 3700 / 10000 loss:  3.03072e-11\n",
      "Step: 3800 / 10000 loss:  1.7342535e-11\n",
      "Step: 3900 / 10000 loss:  9.956665e-12\n",
      "Step: 4000 / 10000 loss:  5.6626557e-12\n",
      "Step: 4100 / 10000 loss:  3.25584e-12\n",
      "Step: 4200 / 10000 loss:  1.8782753e-12\n",
      "Step: 4300 / 10000 loss:  1.0610032e-12\n",
      "Step: 4400 / 10000 loss:  6.1458244e-13\n",
      "Step: 4500 / 10000 loss:  3.514596e-13\n",
      "Step: 4600 / 10000 loss:  1.9691655e-13\n",
      "Step: 4700 / 10000 loss:  1.1461203e-13\n",
      "Step: 4800 / 10000 loss:  6.4392935e-14\n",
      "Step: 4900 / 10000 loss:  3.8968828e-14\n",
      "Step: 5000 / 10000 loss:  2.0206059e-14\n",
      "Step: 5100 / 10000 loss:  1.0843178e-14\n",
      "Step: 5200 / 10000 loss:  7.4014865e-15\n",
      "Step: 5300 / 10000 loss:  4.18184e-15\n",
      "Step: 5400 / 10000 loss:  2.3684759e-15\n",
      "Step: 5500 / 10000 loss:  9.992007e-16\n",
      "Step: 5600 / 10000 loss:  6.2912636e-16\n",
      "Step: 5700 / 10000 loss:  3.330669e-16\n",
      "Step: 5800 / 10000 loss:  2.5905203e-16\n",
      "Step: 5900 / 10000 loss:  2.220446e-16\n",
      "Step: 6000 / 10000 loss:  1.8503717e-16\n",
      "Step: 6100 / 10000 loss:  0.0\n",
      "Step: 6200 / 10000 loss:  0.0\n",
      "Step: 6300 / 10000 loss:  0.0\n",
      "Step: 6400 / 10000 loss:  0.0\n",
      "Step: 6500 / 10000 loss:  0.0\n",
      "Step: 6600 / 10000 loss:  0.0\n",
      "Step: 6700 / 10000 loss:  0.0\n",
      "Step: 6800 / 10000 loss:  0.0\n",
      "Step: 6900 / 10000 loss:  0.0\n",
      "Step: 7000 / 10000 loss:  0.0\n",
      "Step: 7100 / 10000 loss:  0.0\n",
      "Step: 7200 / 10000 loss:  0.0\n",
      "Step: 7300 / 10000 loss:  0.0\n",
      "Step: 7400 / 10000 loss:  0.0\n",
      "Step: 7500 / 10000 loss:  0.0\n",
      "Step: 7600 / 10000 loss:  0.0\n",
      "Step: 7700 / 10000 loss:  0.0\n",
      "Step: 7800 / 10000 loss:  0.0\n",
      "Step: 7900 / 10000 loss:  0.0\n",
      "Step: 8000 / 10000 loss:  0.0\n",
      "Step: 8100 / 10000 loss:  0.0\n",
      "Step: 8200 / 10000 loss:  0.0\n",
      "Step: 8300 / 10000 loss:  0.0\n",
      "Step: 8400 / 10000 loss:  0.0\n",
      "Step: 8500 / 10000 loss:  0.0\n",
      "Step: 8600 / 10000 loss:  0.0\n",
      "Step: 8700 / 10000 loss:  0.0\n",
      "Step: 8800 / 10000 loss:  0.0\n",
      "Step: 8900 / 10000 loss:  0.0\n",
      "Step: 9000 / 10000 loss:  0.0\n",
      "Step: 9100 / 10000 loss:  0.0\n",
      "Step: 9200 / 10000 loss:  0.0\n",
      "Step: 9300 / 10000 loss:  0.0\n",
      "Step: 9400 / 10000 loss:  0.0\n",
      "Step: 9500 / 10000 loss:  0.0\n",
      "Step: 9600 / 10000 loss:  0.0\n",
      "Step: 9700 / 10000 loss:  0.0\n",
      "Step: 9800 / 10000 loss:  0.0\n",
      "Step: 9900 / 10000 loss:  0.0\n",
      "Eigenvector NN = \n",
      " [[-0.39691579]\n",
      " [-0.44182647]\n",
      " [-0.31817309]\n",
      " [-0.44666684]\n",
      " [-0.46830313]\n",
      " [-0.3566427 ]] \n",
      "\n",
      "Eigenvalue NN = \n",
      " [[2.92582919]] \n",
      " \n",
      "\n",
      "Eigenvector analytic = \n",
      " [[ 0.3969158   0.29100653  0.40140726  0.02937581 -0.74911156  0.1860489 ]\n",
      " [ 0.44182644 -0.71132178 -0.16781868 -0.46238523 -0.09608805  0.21820769]\n",
      " [ 0.31817312  0.22136178 -0.78435104  0.11667646 -0.25862122 -0.3925047 ]\n",
      " [ 0.44666685 -0.05238361  0.43638351 -0.05112953  0.26673152 -0.73044793]\n",
      " [ 0.4683031  -0.12182987 -0.01170839  0.73966719  0.31037851  0.34967161]\n",
      " [ 0.35664272  0.58544763 -0.07024875 -0.47117065  0.44185236  0.32846135]]\n",
      "\n",
      "\n",
      "Eigenvalues analytic = \n",
      " [ 2.92582919  0.66100181 -0.45976283  0.39186161 -0.12704888 -0.07943175]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# # Finding eigenvalues of matrices with neural networks. \n",
    "# Script for finding the eigenvectors corresponding to the largest eigenvalue of a matrix with a neural network.\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.reset_default_graph()\n",
    "# tf.set_random_seed(343)\n",
    "\n",
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "#from lib import compute_dx_dt\n",
    "\n",
    "matrix_size = 6\n",
    "\n",
    "A = np.random.random_sample(size=(matrix_size,matrix_size))\n",
    "A = (A.T + A)/2.0\n",
    "start_matrix = A\n",
    "\n",
    "eigen_vals, eigen_vecs =  np.linalg.eig(A)\n",
    "\n",
    "A = tf.convert_to_tensor(A)\n",
    "print(\"A = \", A)\n",
    "\n",
    "x_0 = tf.convert_to_tensor(np.random.random_sample(size = (1,matrix_size)))\n",
    "print(\"x0 = \", x_0)\n",
    "\n",
    "## The construction phase\n",
    "\n",
    "num_iter = 10000\n",
    "num_hidden_neurons = [50]\n",
    "num_hidden_layers = np.size(num_hidden_neurons)\n",
    "\n",
    "\n",
    "with tf.variable_scope('dnn'):\n",
    "\n",
    "    previous_layer = x_0\n",
    "\n",
    "    for l in range(num_hidden_layers):\n",
    "        current_layer = tf.layers.dense(previous_layer, num_hidden_neurons[l],activation=tf.nn.sigmoid)\n",
    "        previous_layer = current_layer\n",
    "\n",
    "    dnn_output = tf.layers.dense(previous_layer, matrix_size)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    print(\"dnn_output = \", dnn_output)\n",
    "    \n",
    "    x_trial = tf.transpose(dnn_output)\n",
    "    print(\"x_trial = \", x_trial)\n",
    "    \n",
    "    temp1 = (tf.tensordot(tf.transpose(x_trial), x_trial, axes=1)*A)\n",
    "    temp2 = (1- tf.tensordot(tf.transpose(x_trial), tf.tensordot(A, x_trial, axes=1), axes=1))*np.eye(matrix_size)\n",
    "    func = tf.tensordot((temp1-temp2), x_trial, axes=1)\n",
    "    \n",
    "    print(temp1)\n",
    "    print(temp2)\n",
    "    print(func)\n",
    "    \n",
    "    func = tf.transpose(func)\n",
    "    x_trial = tf.transpose(x_trial)\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(func, x_trial)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    traning_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "g_dnn = None\n",
    "\n",
    "losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iter):\n",
    "        sess.run(traning_op)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            l = loss.eval()\n",
    "            print(\"Step:\", i, \"/\",num_iter, \"loss: \", l)\n",
    "            losses.append(l)\n",
    "\n",
    "    x_dnn = x_trial.eval()\n",
    "x_dnn = x_dnn.T\n",
    "\n",
    "\n",
    "# ## Plotting loss over time\n",
    "\n",
    "plt.plot(losses[:5])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "print(\"Eigenvector NN = \\n\", (x_dnn/(x_dnn**2).sum()**0.5), \"\\n\")\n",
    "\n",
    "eigen_val_nn = x_dnn.T @ (start_matrix @ x_dnn) / (x_dnn.T @ x_dnn)\n",
    "\n",
    "print(\"Eigenvalue NN = \\n\", eigen_val_nn, \"\\n \\n\")\n",
    "print(\"Eigenvector analytic = \\n\", eigen_vecs)\n",
    "print(\"\\n\")\n",
    "print(\"Eigenvalues analytic = \\n\",eigen_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

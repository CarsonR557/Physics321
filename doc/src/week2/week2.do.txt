TITLE: PHY321: Introduction to Classical Mechanics and plans for Spring 2021
AUTHOR: "Morten Hjorth-Jensen":"http://mhjgit.github.io/info/doc/web/" {copyright, 1999-present|CC BY-NC} at Department of Physics and Astronomy and Facility for Rare Ion Beams (FRIB), Michigan State University, USA & Department of Physics, University of Oslo, Norway
DATE: today


!split
===== Aims and Overview =====

The first week starts on Monday January 11. This week is dedicated to a
review of learning material and reminder on programming aspects,
useful tools, where to find information and much more.

* Monday: Introduction to the course and reminder  on vectors, space, time and motion.
* Wednesday: Python programming reminder, elements from CS201 and how they are used in this course. Installing software (anaconda). See slides at https://mhjensen.github.io/Physics321/doc/pub/week2/html/week2.html). 
* Friday: Introduction to Git and GitHub and getting started with numerical exercises.

Recommended reading: John R. Taylor, Classical Mechanics (Univ. Sci. Books 2005), URL:"https://www.uscibooks.com/taylor2.htm", see also URL:"https://github.com/mhjensen/Physics321/tree/master/doc/Literature". Chapters 1.2 and 1.3 

!split
===== Monday =====

Overview video




!split
===== Introduction =====

Classical mechanics is a topic which has been taught intensively over
several centuries. It is, with its many variants and ways of
presenting the educational material, normally the first _real_ physics
course many of us meet and it lays the foundation for further physics
studies. Many of the equations and ways of reasoning about the
underlying laws of motion and pertinent forces, shape our approaches and understanding
of the scientific method and discourse, as well as the way we develop our insights
and deeper understanding about physical systems.  

There is a wealth of
well-tested (from both a physics point of view and a pedagogical
standpoint) exercises and problems which can be solved
analytically. However, many of these problems represent idealized and
less realistic situations.  The large majority of these problems are
solved by paper and pencil and are traditionally aimed
at what we normally refer to as continuous models from which we may find an analytical solution.  As a consequence,
when teaching mechanics, it implies that we can seldomly venture beyond an idealized case
in order to develop our understandings and insights about the
underlying forces and laws of motion.


!split
===== Space, Time, Motion, Reference Frames  and Reminder on vectors and other mathematical quantities =====

Our studies will start with the motion of different types of objects
such as a falling ball, a runner, a bicycle etc etc. It means that an
object's position in space varies with time.
In order to study such systems we need to define
* choice of origin
* choice of the direction of the axes
* choice of positive direction (left-handed or right-handed system of reference)
* choice of units and dimensions

These choices lead to some important questions such as

* is the  physics of a system independent of the origin of the axes?
* is the  physics independent of the directions of the axes, that is are there privileged axes?
* is the physics independent of the orientation of system?
* is the physics independent of the scale of the length?

=== Dimension, units and labels ===

Throughout this course we will use the standardized SI units. The standard unit for length is thus one meter 1m, for mass
one kilogram 1kg, for time one second 1s, for force one Newton 1kgm/s$^2$ and for energy 1 Joule 1kgm$^2$s$^{-2}$.

We will use the following notations for various variables (vectors are always boldfaced in these lecture notes):
* position $\bm{r}$, in one dimention we will normally just use $x$,
* mass $m$,
* time $t$,
* velocity $\bm{v}$ or just $v$ in one dimension,
* acceleration $\bm{a}$ or just $a$ in one dimension,
* momentum $\bm{p}$ or just $p$ in one dimension,
* kinetic energy $K$,
* potential energy $V$ and
* frequency $\omega$.

More variables will be defined as we need them.

It is also important to keep track of dimensionalities. Don't mix this up with a chosen unit for a given variable. We mark the dimensionality in these lectures as $[a]$, where $a$ is the quantity we are interested in. Thus

* $[\bm{r}]=$ length
* $[m]=$ mass
* $[K]=$ energy
* $[t]=$ time
* $[\bm{v}]=$ length over time
* $[\bm{a}]=$ length over time squared
* $[\bm{p}]=$ mass times length over time
* $[\omega]=$ 1/time


!split
===== Elements of Vector Algebra =====

_Note_: This section is under revision

In these lectures we will use boldfaced lower-case letters to label a vector. A vector $\bm{a}$ in three dimensions is thus defined as
!bt
\[
\bm{a} =(a_x,a_y, a_z),
\]
!et
and using the unit vectors in a cartesian system we have
!bt
\[
\bm{a} = a_x\bm{e}_x+a_y\bm{e}_y+a_z\bm{e}_z,
\]
!et
where the unit vectors have magnitude $\vert\bm{e}_i\vert = 1$ with $i=x,y,z$.

Using the fact that multiplication of reals is distributive we can show that
!bt
\[
\bm{a}(\bm{b}+\bm{c})=\bm{a}\bm{b}+\bm{a}\bm{c},
\]
!et
Similarly we can also show that (using product rule for differentiating reals)
!bt
\[
\frac{d}{dt}(\bm{a}\bm{b})=\bm{a}\frac{d\bm{b}}{dt}+\bm{b}\frac{d\bm{a}}{dt}.
\]
!et

We can repeat these operations for the cross products and show that they are distribuitive
!bt
\[
\bm{a}\times(\bm{b}+\bm{c})=\bm{a}\times\bm{b}+\bm{a}\times\bm{c}.
\]
!et
We have also that
!bt
\[
\frac{d}{dt}(\bm{a}\times\bm{b})=\bm{a}\times\frac{d\bm{b}}{dt}+\bm{b}\times\frac{d\bm{a}}{dt}.
\]
!et
The rotation of a three-dimensional  vector $\bm{a}=(a_x,a_y,a_z)$ in the $xy$ plane around an angle $\phi$ results in a new vector $\bm{b}=(b_x,b_y,b_z)$.  This operation can be expressed in terms of linear algebra as a matrix (the rotation matrix) multiplied with a vector. We can write this as
!bt
\[
\begin{bmatrix} b_x \\ b_y \\ b_z \end{bmatrix} = \begin{bmatrix} \cos{\phi} & \sin{\phi} & 0 \\ -\sin{\phi} & \cos{\phi} & 0 \\ 0 & 0 & 1\end{bmatrix}\begin{bmatrix} a_x \\ a_y \\ a_z \end{bmatrix}.
\]
!et
We can write this in a more compact form as $\bm{b} = \bm{R}\bm{a}$, where the rotation matrix is defined as
!bt
\[
\bm{R} = \begin{bmatrix} \cos{\phi} & \sin{\phi} & 0 \\ -\sin{\phi} & \cos{\phi} & 0 \\ 0 & 0 & 1\end{bmatrix}.
\]
!et




!split
===== Wednesday: Python programming reminder, elements from CS201 and how they are used in this course =====

Overview video

!split
===== Numerical Elements =====
On the other hand, numerical algorithms call for approximate discrete
models and much of the development of methods for continuous models
are nowadays being replaced by methods for discrete models in science and
industry, simply because _much larger classes of problems can be addressed_ with discrete models, often  by simpler and more
generic methodologies.

As we will see below, when properly scaling the equations at hand,
discrete models open up for more advanced abstractions and the possibility to
study  real life systems, with the added bonus that we can explore and
deepen our basic understanding of various physical systems

Analytical solutions are as important as before. In addition, such
solutions provide us with invaluable benchmarks and tests for our
discrete models. Such benchmarks, as we will see below, allow us 
to discuss possible sources of errors and their behaviors.  And
finally, since most of our models are based on various algorithms from
numerical mathematics, we have a unique oppotunity to gain a deeper
understanding of the mathematical approaches we are using.



With computing and data science as important elements in essentially
all aspects of a modern society, we could  then try to define Computing as
_solving scientific problems using all possible tools, including
symbolic computing, computers and numerical algorithms, and analytical
paper and pencil solutions_. 
Computing provides us with the tools to develope our own understanding of the scientific method by enhancing algorithmic thinking.

!split
=====  Computations and the Scientific Method =====

The way we will teach this course reflects
this definition of computing. The course contains both classical paper
and pencil exercises as well as  computational projects and exercises. The
hope is that this will allow you to explore the physics of systems
governed by the degrees of freedom of classical mechanics at a deeper
level, and that these insights about the scientific method will help
you to develop a better understanding of how the underlying forces and
equations of motion and how they impact a given system. Furthermore, by introducing various numerical methods
via computational projects and exercises, we aim at developing your competences and skills about these topics.


These competences will enable you to

* understand how algorithms are used to solve mathematical problems,
* derive, verify, and implement algorithms,
* understand what can go wrong with algorithms,
* use these algorithms to construct reproducible scientific outcomes and to engage in science in ethical ways, and
* think algorithmically for the purposes of gaining deeper insights about scientific problems.

All these elements are central for maturing and gaining a better understanding of the modern scientific process *per se*.

The power of the scientific method lies in identifying a given problem
as a special case of an abstract class of problems, identifying
general solution methods for this class of problems, and applying a
general method to the specific problem (applying means, in the case of
computing, calculations by pen and paper, symbolic computing, or
numerical computing by ready-made and/or self-written software). This
generic view on problems and methods is particularly important for
understanding how to apply available, generic software to solve a
particular problem.

*However, verification of algorithms and understanding their limitations requires much of the classical knowledge about continuous models.*


!split
===== A well-known examples to illustrate many of the above concepts =====

Before we venture into a reminder on Python and mechanics relevant applications, let us briefly outline some of the
abovementioned topics using an example many of you may have seen before in for example CMSE201. 
A simple algorithm for integration is the Trapezoidal rule. 
Integration of a function $f(x)$ by the Trapezoidal Rule is given by following algorithm for an interval $x \in [a,b]$
!bt
\[
  \int_a^b(f(x) dx = \frac{1}{2}\left [f(a)+2f(a+h)+\dots+2f(b-h)+f(b)\right] +O(h^2),
\]
!et
where $h$ is the so-called stepsize defined by the number of integration points $N$ as $h=(b-a)/(n)$.
Python offers an  extremely versatile programming  environment, allowing for
the inclusion of analytical studies in a numerical program. Here we show an
example code with the _trapezoidal rule_. We use also _SymPy_ to evaluate the exact value of the integral and compute the absolute error
with respect to the numerically evaluated one of the integral
$\int_0^1 dx x^2 = 1/3$.
The following code for  the trapezoidal rule allows you  to plot the relative error by comparing with the exact result. By increasing to $10^8$ points one arrives at a region where numerical errors start to accumulate.
!bc pycod
from math import log10
import numpy as np
from sympy import Symbol, integrate
import matplotlib.pyplot as plt
# function for the trapezoidal rule
def Trapez(a,b,f,n):
   h = (b-a)/float(n)
   s = 0
   x = a
   for i in range(1,n,1):
       x = x+h
       s = s+ f(x)
   s = 0.5*(f(a)+f(b)) +s
   return h*s
#  function to compute pi
def function(x):
    return x*x
# define integration limits
a = 0.0;  b = 1.0;
# find result from sympy
# define x as a symbol to be used by sympy
x = Symbol('x')
exact = integrate(function(x), (x, a, b))
# set up the arrays for plotting the relative error
n = np.zeros(9); y = np.zeros(9);
# find the relative error as function of integration points
for i in range(1, 8, 1):
    npts = 10**i
    result = Trapez(a,b,function,npts)
    RelativeError = abs((exact-result)/exact)
    n[i] = log10(npts); y[i] = log10(RelativeError);
plt.plot(n,y, 'ro')
plt.xlabel('n')
plt.ylabel('Relative error')
plt.show()
!ec


!split
===== Analyzing the above example =====
This example shows the potential of combining numerical algorithms with symbolic calculations, allowing us to 

* Validate and verify  their  algorithms. 
* Including concepts like unit testing, one has the possibility to test and test several or all parts of the code.
* Validation and verification are then included *naturally* and one can develop a better attitude to what is meant with an ethically sound scientific approach.
* The above example allows the student to also test the mathematical error of the algorithm for the trapezoidal rule by changing the number of integration points. The students get _trained from day one to think error analysis_. 
* With a Jupyter notebook you can keep exploring similar examples and turn them in as your own notebooks. 


In this process we can easily bake in
 o   How to structure a code in terms of functions
 o   How to make a module
 o   How to read input data flexibly from the command line
 o   How to create graphical/web user interfaces
 o   How to write unit tests (test functions or doctests)
 o   How to refactor code in terms of classes (instead of functions only)
 o   How to conduct and automate large-scale numerical experiments
 o   How to write scientific reports in various formats (LaTeX, HTML)


The conventions and techniques outlined here will save you a lot of time when you incrementally extend software over time from simpler to more complicated problems. In particular, you will benefit from many good habits:
 o New code is added in a modular fashion to a library (modules)
 o Programs are run through convenient user interfaces
 o It takes one quick command to let all your code undergo heavy testing 
 o Tedious manual work with running programs is automated,
 o Your scientific investigations are reproducible, scientific reports with top quality typesetting are produced both for paper and electronic devices.




!split
===== Python practicalities, Software and needed installations =====

We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
Jupyter notebooks invaluable in your work.  

If you have Python installed (we strongly recommend Python3) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via _pip_ as 

o pip install numpy scipy matplotlib ipython scikit-learn mglearn sympy pandas pillow 

For Python3, replace _pip_ with _pip3_.

For OSX users we recommend, after having installed Xcode, to
install _brew_. Brew allows for a seamless installation of additional
software via for example 

o brew install python3

For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use _pip_ as well and simply install Python as 

o sudo apt-get install python3  (or python for pyhton2.7)

etc etc. 



===== Python installers =====

If you don't want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely 

* "Anaconda":"https://docs.anaconda.com/", 

which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system _conda_. 

* "Enthought canopy":"https://www.enthought.com/product/canopy/" 

is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.

Furthermore, "Google's Colab":"https://colab.research.google.com/notebooks/welcome.ipynb" is a free Jupyter notebook environment that requires 
no setup and runs entirely in the cloud. Try it out!

===== Useful Python libraries =====
Here we list several useful Python libraries we strongly recommend (if you use anaconda many of these are already there)

* "NumPy":"https://www.numpy.org/" is a highly popular library for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays
* "The pandas":"https://pandas.pydata.org/" library provides high-performance, easy-to-use data structures and data analysis tools 
* "Xarray":"http://xarray.pydata.org/en/stable/" is a Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun!
* "Scipy":"https://www.scipy.org/" (pronounced “Sigh Pie”) is a Python-based ecosystem of open-source software for mathematics, science, and engineering. 
* "Matplotlib":"https://matplotlib.org/" is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.
* "Autograd":"https://github.com/HIPS/autograd" can automatically differentiate native Python and Numpy code. It can handle a large subset of Python's features, including loops, ifs, recursion and closures, and it can even take derivatives of derivatives of derivatives
* "SymPy":"https://www.sympy.org/en/index.html" is a Python library for symbolic mathematics. 
* "scikit-learn":"https://scikit-learn.org/stable/" has simple and efficient tools for machine learning, data mining and data analysis
* "TensorFlow":"https://www.tensorflow.org/" is a Python library for fast numerical computing created and released by Google
* "Keras":"https://keras.io/" is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano
* And many more such as "pytorch":"https://pytorch.org/",  "Theano":"https://pypi.org/project/Theano/" etc 


Your jupyter notebook can easily be
converted into a nicely rendered _PDF_ file or a Latex file for
further processing. For example, convert to latex as 

!bc 
pycod jupyter nbconvert filename.ipynb --to latex 
!ec

And to add more versatility, the Python package "SymPy":"http://www.sympy.org/en/index.html" is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS)  and is entirely written in Python. 



===== Numpy examples and Important Matrix and vector handling packages =====

There are several central software libraries for linear algebra and eigenvalue problems. Several of the more
popular ones have been wrapped into ofter software packages like those from the widely used text _Numerical Recipes_. The original source codes in many of the available packages are often taken from the widely used
software package LAPACK, which follows two other popular packages
developed in the 1970s, namely EISPACK and LINPACK.  We describe them shortly here.

  * LINPACK: package for linear equations and least square problems.
  * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website URL: "http://www.netlib.org" it is possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.
  * BLAS (I, II and III): (Basic Linear Algebra Subprograms) are routines that provide standard building blocks for performing basic vector and matrix operations. Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. Highly parallelized and efficient codes, all available for download from URL: "http://www.netlib.org".


===== Basic Matrix Features =====

!bblock Matrix properties reminder
!bt
\[
 \mathbf{A} =
      \begin{bmatrix} a_{11} & a_{12} & a_{13} & a_{14} \\
                                 a_{21} & a_{22} & a_{23} & a_{24} \\
                                   a_{31} & a_{32} & a_{33} & a_{34} \\
                                  a_{41} & a_{42} & a_{43} & a_{44}
             \end{bmatrix}\qquad
\mathbf{I} =
      \begin{bmatrix} 1 & 0 & 0 & 0 \\
                                 0 & 1 & 0 & 0 \\
                                 0 & 0 & 1 & 0 \\
                                 0 & 0 & 0 & 1
             \end{bmatrix}
\]
!et



The inverse of a matrix is defined by

!bt
\[
\mathbf{A}^{-1} \cdot \mathbf{A} = I
\]
!et


|----------------------------------------------------------------------|
|       Relations      |       Name      | matrix elements             |
|----------------------------------------------------------------------|
| $A = A^{T}$         | symmetric       | $a_{ij} = a_{ji}$            |
| $A = \left (A^{T} \right )^{-1}$ | real orthogonal | $\sum_k a_{ik} a_{jk} = \sum_k a_{ki} a_{kj} = \delta_{ij}$ |
| $A = A^{ * }$          | real matrix     | $a_{ij} = a_{ij}^{ * }$       |
| $A = A^{\dagger}$     |  hermitian      | $a_{ij} = a_{ji}^{ * }$       |
| $A = \left (A^{\dagger} \right )^{-1}$ | unitary | $\sum_k a_{ik} a_{jk}^{ * } = \sum_k a_{ki}^{ * } a_{kj} = \delta_{ij}$ |
|----------------------------------------------------------------------|

!eblock


=== Some famous Matrices ===

  * Diagonal if $a_{ij}=0$ for $i\ne j$
  * Upper triangular if $a_{ij}=0$ for $i > j$
  * Lower triangular if $a_{ij}=0$ for $i < j$
  * Upper Hessenberg if $a_{ij}=0$ for $i > j+1$
  * Lower Hessenberg if $a_{ij}=0$ for $i < j+1$
  * Tridiagonal if $a_{ij}=0$ for $|i -j| > 1$
  * Lower banded with bandwidth $p$: $a_{ij}=0$ for $i > j+p$
  * Upper banded with bandwidth $p$: $a_{ij}=0$ for $i < j+p$
  * Banded, block upper triangular, block lower triangular....


=== More Basic Matrix Features ===

!bblock Some Equivalent Statements
For an $N\times N$ matrix  $\mathbf{A}$ the following properties are all equivalent

  * If the inverse of $\mathbf{A}$ exists, $\mathbf{A}$ is nonsingular.
  * The equation $\mathbf{Ax}=0$ implies $\mathbf{x}=0$.
  * The rows of $\mathbf{A}$ form a basis of $R^N$.
  * The columns of $\mathbf{A}$ form a basis of $R^N$.
  * $\mathbf{A}$ is a product of elementary matrices.
  * $0$ is not eigenvalue of $\mathbf{A}$.
!eblock


===== Numpy and arrays =====
"Numpy":"http://www.numpy.org/" provides an easy way to handle arrays in Python. The standard way to import this library is as

!bc pycod
import numpy as np
!ec
Here follows a simple example where we set up an array of ten elements, all determined by random numbers drawn according to the normal distribution,
!bc pycod
n = 10
x = np.random.normal(size=n)
print(x)
!ec
We defined a vector $x$ with $n=10$ elements with its values given by the Normal distribution $N(0,1)$.
Another alternative is to declare a vector as follows
!bc pycod
import numpy as np
x = np.array([1, 2, 3])
print(x)
!ec
Here we have defined a vector with three elements, with $x_0=1$, $x_1=2$ and $x_2=3$. Note that both Python and C++
start numbering array elements from $0$ and on. This means that a vector with $n$ elements has a sequence of entities $x_0, x_1, x_2, \dots, x_{n-1}$. We could also let (recommended) Numpy to compute the logarithms of a specific array as
!bc pycod
import numpy as np
x = np.log(np.array([4, 7, 8]))
print(x)
!ec

In the last example we used Numpy's unary function $np.log$. This function is
highly tuned to compute array elements since the code is vectorized
and does not require looping. We normaly recommend that you use the
Numpy intrinsic functions instead of the corresponding _log_ function
from Python's _math_ module. The looping is done explicitely by the
_np.log_ function. The alternative, and slower way to compute the
logarithms of a vector would be to write

!bc pycod
import numpy as np
from math import log
x = np.array([4, 7, 8])
for i in range(0, len(x)):
    x[i] = log(x[i])
print(x)
!ec
We note that our code is much longer already and we need to import the _log_ function from the _math_ module. 
The attentive reader will also notice that the output is $[1, 1, 2]$. Python interprets automagically our numbers as integers (like the _automatic_ keyword in C++). To change this we could define our array elements to be double precision numbers as
!bc pycod
import numpy as np
x = np.log(np.array([4, 7, 8], dtype = np.float64))
print(x)
!ec
or simply write them as double precision numbers (Python uses 64 bits as default for floating point type variables), that is
!bc pycod
import numpy as np
x = np.log(np.array([4.0, 7.0, 8.0])
print(x)
!ec
To check the number of bytes (remember that one byte contains eight bits for double precision variables), you can use simple use the _itemsize_ functionality (the array $x$ is actually an object which inherits the functionalities defined in Numpy) as 
!bc pycod
import numpy as np
x = np.log(np.array([4.0, 7.0, 8.0])
print(x.itemsize)
!ec


===== Matrices in Python =====

Having defined vectors, we are now ready to try out matrices. We can
define a $3 \times 3 $ real matrix $\hat{A}$ as (recall that we user
lowercase letters for vectors and uppercase letters for matrices)

!bc pycod
import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
print(A)
!ec
If we use the _shape_ function we would get $(3, 3)$ as output, that is verifying that our matrix is a $3\times 3$ matrix. We can slice the matrix and print for example the first column (Python organized matrix elements in a row-major order, see below) as
!bc pycod
import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
# print the first column, row-major order and elements start with 0
print(A[:,0]) 
!ec
We can continue this was by printing out other columns or rows. The example here prints out the second column
!bc pycod
import numpy as np
A = np.log(np.array([ [4.0, 7.0, 8.0], [3.0, 10.0, 11.0], [4.0, 5.0, 7.0] ]))
# print the first column, row-major order and elements start with 0
print(A[1,:]) 
!ec 
Numpy contains many other functionalities that allow us to slice, subdivide etc etc arrays. We strongly recommend that you look up the "Numpy website for more details":"http://www.numpy.org/". Useful functions when defining a matrix are the _np.zeros_ function which declares a matrix of a given dimension and sets all elements to zero
!bc pycod
import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to zero
A = np.zeros( (n, n) )
print(A) 
!ec 
or initializing all elements to 
!bc pycod
import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to one
A = np.ones( (n, n) )
print(A) 
!ec 
or as unitarily distributed random numbers (see the material on random number generators in the statistics part)
!bc pycod
import numpy as np
n = 10
# define a matrix of dimension 10 x 10 and set all elements to random numbers with x \in [0, 1]
A = np.random.rand(n, n)
print(A) 
!ec 


===== Meet the Pandas =====


FIGURE: [fig/pandas.jpg, width=600 frac=0.8]

Another useful Python package is
"pandas":"https://pandas.pydata.org/", which is an open source library
providing high-performance, easy-to-use data structures and data
analysis tools for Python. _pandas_ stands for panel data, a term borrowed from econometrics and is an efficient library for data analysis with an emphasis on tabular data.
_pandas_ has two major classes, the _DataFrame_ class with two-dimensional data objects and tabular data organized in columns and the class _Series_ with a focus on one-dimensional data objects. Both classes allow you to index data easily as we will see in the examples below. 
_pandas_ allows you also to perform mathematical operations on the data, spanning from simple reshapings of vectors and matrices to statistical operations. 

The following simple example shows how we can, in an easy way make tables of our data. Here we define a data set which includes names, place of birth and date of birth, and displays the data in an easy to read way. We will see repeated use of _pandas_, in particular in connection with classification of data. 

!bc pycod 
import pandas as pd
from IPython.display import display
data = {'First Name': ["Frodo", "Bilbo", "Aragorn II", "Samwise"],
        'Last Name': ["Baggins", "Baggins","Elessar","Gamgee"],
        'Place of birth': ["Shire", "Shire", "Eriador", "Shire"],
        'Date of Birth T.A.': [2968, 2890, 2931, 2980]
        }
data_pandas = pd.DataFrame(data)
display(data_pandas)
!ec

In the above we have imported _pandas_ with the shorthand _pd_, the latter has become the standard way we import _pandas_. We make then a list of various variables
and reorganize the above lists into a _DataFrame_ and then print out  a neat table with specific column labels as *Name*, *place of birth* and *date of birth*.
Displaying these results, we see that the indices are given by the default numbers from zero to three.
_pandas_ is extremely flexible and we can easily change the above indices by defining a new type of indexing as
!bc pycod
data_pandas = pd.DataFrame(data,index=['Frodo','Bilbo','Aragorn','Sam'])
display(data_pandas)
!ec
Thereafter we display the content of the row which begins with the index _Aragorn_
!bc pycod
display(data_pandas.loc['Aragorn'])
!ec

We can easily append data to this, for example
!bc pycod
new_hobbit = {'First Name': ["Peregrin"],
              'Last Name': ["Took"],
              'Place of birth': ["Shire"],
              'Date of Birth T.A.': [2990]
              }
data_pandas=data_pandas.append(pd.DataFrame(new_hobbit, index=['Pippin']))
display(data_pandas)
!ec


Here are other examples where we use the _DataFrame_ functionality to handle arrays, now with more interesting features for us, namely numbers. We set up a matrix 
of dimensionality $10\times 5$ and compute the mean value and standard deviation of each column. Similarly, we can perform mathematial operations like squaring the matrix elements and many other operations. 
!bc pycod
import numpy as np
import pandas as pd
from IPython.display import display
np.random.seed(100)
# setting up a 10 x 5 matrix
rows = 10
cols = 5
a = np.random.randn(rows,cols)
df = pd.DataFrame(a)
display(df)
print(df.mean())
print(df.std())
display(df**2)
!ec

Thereafter we can select specific columns only and plot final results
!bc pycod
df.columns = ['First', 'Second', 'Third', 'Fourth', 'Fifth']
df.index = np.arange(10)

display(df)
print(df['Second'].mean() )
print(df.info())
print(df.describe())

from pylab import plt, mpl
plt.style.use('seaborn')
mpl.rcParams['font.family'] = 'serif'

df.cumsum().plot(lw=2.0, figsize=(10,6))
plt.show()


df.plot.bar(figsize=(10,6), rot=15)
plt.show()
!ec
We can produce a $4\times 4$ matrix
!bc pycod
b = np.arange(16).reshape((4,4))
print(b)
df1 = pd.DataFrame(b)
print(df1)
!ec
and many other operations. 

The _Series_ class is another important class included in
_pandas_. You can view it as a specialization of _DataFrame_ but where
we have just a single column of data. It shares many of the same features as _DataFrame. As with _DataFrame_,
most operations are vectorized, achieving thereby a high performance when dealing with computations of arrays, in particular labeled arrays.
As we will see below it leads also to a very concice code close to the mathematical operations we may be interested in.
For multidimensional arrays, we recommend strongly "xarray":"http://xarray.pydata.org/en/stable/". _xarray_ has much of the same flexibility as _pandas_, but allows for the extension to higher dimensions than two.




!split
===== Friday: Introduction to Git and GitHub and getting started with numerical exercises =====





\section{Math Basics}

\subsection{Scalars, Vectors and Matrices}

A scalar is something with a value that is independent of coordinate
system. Examples are mass, or the relative time between events. A
vector has magnitude and direction. Under rotation, the magnitude
stays the same but the direction changes. Scalars have no spatial
index, whereas a three-dimensional vector has 3 indices, e.g. the
position $\vec{r}$ has components $r_1,r_2,r_3$, which are often
referred to as $x,y,z$.

There are several categories of changes of coordinate system. The
observer can translate the origin, might move with a different
velocity, or might rotate his/her coordinate axes. For instance, a
particle's position vector changes when the origin is translated, but
its velocity does not. When you study relativity you will find that
quantities you thought of as scalars, such as time or electric
potential, are actually parts of four-dimensional vectors and that
changes of the velocity of the reference frame act in a similar way to
rotations.

In addition to vectors and scalars, there are matrices, which have two
indices. One also has objects with 3 or four indices. These are called
tensors of rank $n$, where $n$ is the number of indices. A matrix is a
rank-two tensor. The Levi-Civita symbol, $\epsilon_{ijk}$ used for
cross products, is a third-rank tensor.

\subsubsection*{Unit Vectors}

Also known as basis vectors, unit vectors point in the direction of
the coordinate axes, have unit norm, and are orthogonal to one
another. Sometimes this is referred to as an orthonormal basis,

!bt
\begin{equation}
\hat{e}_i\cdot\hat{e}_j=\delta_{ij}=\left(\begin{array}{ccc}
1 & 0 & 0\\
0& 1 & 0\\
0 & 0 & 1
\end{array}\right).
\end{equation}
!et

Here, $\delta_{ij}$ is unity when $i=j$ and is zero otherwise. This is
called the unit matrix, because you can multiply it with any other
matrix and not change the matrix. The ``dot'' denotes the dot product,
$\vec{A}\cdot\vec{B}=A_1B_1+A_2B_2+A_3B_3=|A||B|\cos\theta_{AB}$. Sometimes
the unit vectors are called $\hat{x}$, $\hat{y}$ and
$\hat{z}$. Vectors can be decomposed in terms of unit vectors,
\begin{equation} \vec{r}=r_1\hat{e}_1+r_2\hat{e}_2+r_3\hat{e}_3.
\end{equation} The vector components $r_1$, $r_2$ and $r_3$ might be
called $x$, $y$ and $z$ for a displacement of $v_x$, $v_y$ and $v_z$
for a velocity.

\subsubsection*{Rotations}

Here, we use rotations as an example of matrices and their operations. One can consider a different orthonormal basis $\hat{e}'_1$, $\hat{e}'_2$ and $\hat{e}'_3$. The same vector $\vec{r}$ mentioned above can also be expressed in the new basis,

!bt
\begin{equation}
\vec{r}=r'_1\hat{e}'_1+r'_2\hat{e}'_2+r'_3\hat{e}'_3.
\end{equation}
!et

Even though it is the same vector, the components have changed. Each new unit vector $\hat{e}'_i$ can be expressed as a linear sum of the previous vectors,

!bt
\begin{equation}
\hat{e}'_i=\sum_j U_{ij}\hat{e}_j,
\end{equation}
!et

and the matrix $U$ can be found by taking the dot product of both sides with $\hat{e}_k$,

!bt
\begin{eqnarray}
\nonumber
\hat{e}_k\cdot\hat{e}'_i&=&\sum_jU_{ij}\hat{e}_k\cdot\hat{e}_j\\
\label{eq:lambda_angles}
\hat{e}_k\cdot\hat{e}'_i&=&\sum_jU_{ij}\delta_{jk}=U_{ik}.
\end{eqnarray}
!et

Thus, the matrix lambda has components $U_{ij}$ that are equal to the
cosine of the angle between new unit vector $\hat{e}'_i$ and the old
unit vector $\hat{e}_j$.


!bt
\begin{equation}
U = \left(\begin{array}{ccc}
\hat{e}'_1\cdot\hat{e}_1& \hat{e}'_1\cdot\hat{e}_2& \hat{e}'_1\cdot\hat{e}_3\\
\hat{e}'_2\cdot\hat{e}_1& \hat{e}'_2\cdot\hat{e}_2& \hat{e}'_2\cdot\hat{e}_3\\
\hat{e}'_3\cdot\hat{e}_1& \hat{e}'_3\cdot\hat{e}_2& \hat{e}'_3\cdot\hat{e}_3
\end{array}\right),~~~~~U_{ij}=\hat{e}'_i\cdot\hat{e}_j=\cos\theta_{ij}.
\end{equation}
!et

Note that the matrix is not symmetric, $U_{ij}\ne U_{ji}$. One can also look at the inverse transformation, by switching the primed and unprimed coordinates,

!bt
\begin{eqnarray}
\label{eq:inverseU}
\hat{e}_i&=&\sum_jU^{-1}_{ij}\hat{e}'_j,\\
\nonumber
U^{-1}_{ij}&=&\hat{e}_i\cdot\hat{e}'_j=U_{ji}.
\end{eqnarray}
!et

The definition of transpose of a matrix, $M^{t}_{ij}=M_{ji}$, allows one to state this as

!bt
\begin{eqnarray}
\label{eq:transposedef}
U^{-1}&=&U^{t}.
\end{eqnarray}
!et

A tensor obeying Eq. (\ref{eq:transposedef}) defines what is known as
a unitary, or orthogonal, transformation.

The matrix $U$ can be used to transform any vector to the new basis. Consider a vector

!bt
\begin{eqnarray}
\vec{r}&=&r_1\hat{e}_1+r_2\hat{e}_2+r_3\hat{e}_3\\
\nonumber
&=&r'_1\hat{e}'_1+r'_2\hat{e}'_2+r'_3\hat{e}'_3.
\end{eqnarray}
!et

This is the same vector expressed as a sum over two different sets of
basis vectors. The coefficients $r_i$ and $r'_i$ represent components
of the same vector. The relation between them can be found by taking
the dot product of each side with one of the unit vectors,
$\hat{e}_i$, which gives

!bt
\begin{eqnarray}
r_i&=&\sum_j \hat{e}_i\cdot\hat{e}'_j~r'_j.
\end{eqnarray}
!et

Using Eq. (\ref{eq:inverseU}) one can see that the transformation of $r$ can be also written in terms of $U$,

!bt
\begin{eqnarray}
\label{eq:rotateR}
r_i&=&\sum_jU^{-1}_{ij}~r'_j.
\end{eqnarray}
!et

Thus, the matrix that transforms the coordinates of the unit vectors,
Eq. (\ref{eq:inverseU}) is the same one that transforms the
coordinates of a vector, Eq. (\ref{eq:rotateR}).


Find the rotation matrix $U$ for finding the components in the primed coordinate system given from those in the unprimed system, given that the unit vectors in the new system are found by rotating the coordinate system by and angle $\phi$ about the $z$ axis.

In this case

!bt
\begin{eqnarray*}
\hat{e}'_1&=&\cos\phi \hat{e}_1-\sin\phi\hat{e}_2,\\
\hat{e}'_2&=&\sin\phi\hat{e}_1+\cos\phi\hat{e}_2,\\
\hat{e}'_3&=&\hat{e}_3.
\end{eqnarray*}
!et
By inspecting Eq. (\ref{eq:lambda_angles}),

!bt
\[
U=\left(\begin{array}{ccc}
\cos\phi&-\sin\phi&0\\
\sin\phi&\cos\phi&0\\
0&0&1\end{array}\right).
\]
!et

Under a unitary transformation $U$ (or basis transformation) scalars
are unchanged, whereas vectors $\vec{r}$ and matrices $M$ change as

!bt
\begin{eqnarray}
r'_i&=&U_{ij}~ r_j, ~~({\rm sum~inferred})\\
\nonumber
M'_{ij}&=&U_{ik}M_{km}U^{-1}_{mj}.
\end{eqnarray}
!et

Physical quantities with no spatial indices are scalars (or
pseudoscalars if they depend on right-handed vs. left-handed
coordinate systems), and are unchanged by unitary
transformations. This includes quantities like the trace of a matrix,
the matrix itself had indices but none remain after performing the
trace.

!bt
\begin{eqnarray}
{\rm Tr} M&\equiv& M_{ii}.
\end{eqnarray}
!et

Because there are no remaining indices, one expects it to be a scalar. Indeed one can see this,

!bt
\begin{eqnarray}
{\rm Tr} M'&=&U_{ij}M_{jm}U^{-1}_{mi}\\
\nonumber
&=&M_{jm}U^{-1}_{mi}U_{ij}\\
\nonumber
&=&M_{jm}\delta_{mj}\\
\nonumber
&=&M_{jj}={\rm Tr} M.
\end{eqnarray}
!et

A similar example is the determinant of a matrix, which is also a scalar.

\subsubsection*{Vector and Matrix Operations}


\begin{itemize}\itemsep=0pt
\item Scalar Product (or dot product): For vectors $\vec{A}$ and $\vec{B}$,


!bt
\begin{eqnarray*}
\vec{A}\cdot\vec{B}&=&A_iB_i=|A||B|\cos\theta_{AB},\\
|A|&\equiv& \sqrt{\vec{A}\cdot\vec{A}}.
\end{eqnarray*}
!et

Note that the summation sign is inferred any time there are repeated indices. For example, 

!bt
\begin{eqnarray}
A_iB_i&=&\sum A_iB_i.
\end{eqnarray}
!et

\item Multiplying a matrix $C$ times a vector $\vec{A}$:

!bt
\[
(CA)_i=C_{ij}A_j.
\]
!et

For the $i^{\rm th}$ element one takes the scalar product of the $i^{\rm th}$ row of $C$ with the vector $\vec{A}$.
\item Mutiplying matrices $C$ and $D$:

!bt
\[
(CD)_{ij}=C_{ik}D_{kj},
\]
!et

This means the one obtains the $ij$ element by taking the scalar product of the $i^{\rm th}$ of $C$ with the $j^{\rm th}$ column of $D$.
\item Vector Product (or cross product) of vectors $\vec{A}$ and $\vec{B}$:

!bt
\begin{eqnarray*}
\vec{C}&=&\vec{A}\times\vec{B},\\
C_i&=&\epsilon_{ijk}A_jB_k.
\end{eqnarray*}
!et

Here $\epsilon$ is the third-rank anti-symmetric tensor, also known as the Levi-Civita symbol. It is $\pm 1$ only if all three indices are different, and is zero otherwise. The choice of $\pm 1$ depends on whether the indices are an even or odd permutation of the original symbols. The permutation $xyz$ or $123$ is considered to be $+1$. For the 27 elements,
\begin{eqnarray}
\epsilon_{ijk}&=&-\epsilon_{ikj}=-\epsilon_{jik}=-\epsilon_{kji}\\
\nonumber
\epsilon_{123}&=&\epsilon_{231}=\epsilon_{312}=1,\\
\nonumber
\epsilon_{213}&=&\epsilon_{132}=\epsilon_{321}=-1,\\
\nonumber
\epsilon_{iij}&=&\epsilon_{iji}=\epsilon_{jii}=0.
\end{eqnarray}
You used cross products extensively when studying magnetic fields. Because the matrix is anti-symmetric, switching the $x$ and $y$ axes (or any two axes) flips the sign. If the coordinate system is right-handed, meaning the $xyz$ axes satisfy $\hat{x}\times\hat{y}=\hat{z}$, where you can point along the $x$ axis with your extended right index finger, the $y$ axis with your contracted middle finger and the $z$ axis with your extended thumb. Switching to a left-handed system flips the sign of the vector $\vec{C}=\vec{A}\times\vec{B}$. Note that $\vec{A}\times\vec{B}=-\vec{B}\times\vec{A}$. The vector $\vec{C}$ is perpendicular to both $\vec{A}$ and $\vec{B}$ and the magnitude of $\vec{C}$ is given by 
\[
|C|=|A||B|\sin\theta_{AB}.
\]
Vectors obtained by the cross product of two real vectors are called pseudo-vectors because the assignment of their direction can be arbitrarily flipped by defining the Levi-Civita symbol to be based on left-handed rules. Examples are the magnetic field and angular momentum. If the direction of a real vector prefers the right-handed over the left-handed direction, that constitutes a violation of parity. For instance, one can polarize the spins (angular momentum) of nuclei with a magnetic field so that the spins preferentially point along the direction of the magnetic field. This does not violate parity because both are pseudo-vectors. Now assume these polarized nuclei decay and that electrons are one of the products. If these electrons prefer to exit the decay parallel vs. antiparallel to the polarizing magnetic field, this constitutes parity violation because the direction of the outgoing electron momenta are a real vector. This is precisely what is observed in weak decays.

\item Differentiation of a vector with respect to a scalar: For example, the acceleration is $d\vec{v}/dt$:
\[
(d\vec{v}/dt)_i=\frac{dv_i}{dt}.
\]

\item Angular velocity: Choose the vector $\vec{\omega}$ so that the motion is like your fingers wrapping around your thumb on your right hand with $\vec{\omega}$ pointing along your thumb. The magnitude is $d|\phi|/dt$.

\item Gradient operator $\nabla$: This is the derivative $\partial/\partial x$, $\partial/\partial y$ and $\partial/\partial z$, where $\partial _x$ means $\partial/\partial_x$. For taking the gradient of a scalar $\Phi$,
\[
{\rm\bf grad}~\Phi, (\nabla\Phi(x,y,z,t))_i=\partial/\partial r_i\Phi(\vec{r},t)=\partial_i\Phi(\vec{r},t).
\]
For taking the dot product of the gradient with a vector, sometimes called a divergence,
\[
div \vec{A}, \nabla\cdot\vec{A}=\partial_i A_i.
\]
For taking the vector product with another vector, sometimes called curl, $\nabla\times\vec{A}$,
\[
{\rm\bf curl}~\vec{A}, (\nabla\times\vec{A})_i=\epsilon_{ijk}\partial_j A_k(\vec{r},t).
\]
\item The Laplacian is referred to as $\nabla^2$ and is defined as
\[
\nabla^2=\nabla\cdot\nabla=\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2}.
\]
\end{itemize}

\subsubsection*{Some identities}

Here we simply state these, but you may wish to prove a few. They are useful for this class and will be essential when you study E\&M.
\begin{eqnarray}
\vec{A}\cdot(\vec{B}\times\vec{C})&=&\vec{B}\cdot(\vec{C}\times\vec{A})=\vec{C}\cdot(\vec{A}\times\vec{B})\\
\nonumber
\vec{A}\times(\vec{B}\times\vec{C})&=&(\vec{A}\cdot\vec{C})\vec{B}-(\vec{A}\cdot\vec{B})\vec{C}\\
\nonumber
(\vec{A}\times\vec{B})\cdot(\vec{C}\times\vec{D})&=&(\vec{A}\cdot\vec{C})(\vec{B}\cdot\vec{D})
-(\vec{A}\cdot\vec{D})(\vec{B}\cdot\vec{C})
\end{eqnarray}



\example
The height of a hill is given by the formula 
\[
z(x,y)=2xy-3x^2-4y^2-18x+28y+12.
\]
Here $z$ is the height in meters and $x$ and $y$ are the east-west and north-south coordinates. Find the position $x,y$ where the hill is the highest, and give its height.

{\bf Solution:} The maxima or minima, or inflection points, are given when $\partial_x z=0$ and $\partial_yz=0$.
\begin{eqnarray*}
\partial_xz(x,y)&=&2y-6x-18=0,\\
\partial_yz(x,y)&=&2x-8y+28=0.
\end{eqnarray*}
Solving for two equations and two unknowns gives one solution $x=-2,y=3$. This then gives $z=72$. Although this procedure could have given a minimum or an inflection point you can look at the form for $z$ and see that for $x=y=0$ the height is lower, therefore it is not a minimum. You can also look and see that the quadratic contributions are negative so the height falls off to $-\infty$ far away in any direction. Thus, there must be a maximum, and this must be it. Deciding between maximum or minimum or inflection point for a general problem would involve looking at the matrix $\partial_i\partial_j z$ at the specific point, then finding the eigenvalues of the matrix and seeing whether they were both positive (minimum) both negative (maximum) or one positive and one negative (saddle point).

\exampleend

\subsubsection*{Gauss's Theorem and Stokes's Theorem}

For an integral over a volume $V$ confined by a surface $S$, Gauss's theorem gives
\[
\int_V dv~\nabla\cdot\vec{A}=\int_Sd\vec{S}\cdot\vec{A}.
\]
For a closed path $C$ which carves out some area $S$, 
\[
\int_C d\vec{\ell}\cdot\vec{A}=\int_Sd\vec{s} \cdot(\nabla\times\vec{A})
\]
Stoke's law can be understood by considering a small rectangle, $-\Delta x<x<\Delta x$, $-\Delta y<y<\Delta y$. The path integral around the edges is
\begin{eqnarray}
\int_C d\vec{\ell}\cdot\vec{A}&=&2\Delta y[A_y(\Delta x,0)-A_y(-\Delta x,0)]-2\Delta x[A_x(0,\Delta y)-A_x(0,-\Delta y)]\\
\nonumber
&=&4\Delta x\Delta y\left\{
\frac{A_y(\Delta x,0)-A_y(-\Delta x,0)}{2\Delta x}-\frac{A_x(0,\Delta y)-A_x(0,-\Delta y)}{2\Delta y}\right\}\\
\nonumber
&=&4\Delta x\Delta y\left\{\frac{\partial A_y}{\partial x}-\frac{\partial A_x}{\partial y}\right\}\\
&=&\Delta S \cdot \nabla\times\vec{A}.
\end{eqnarray}
Here $\Delta S$ is the area of the surface element.

\subsubsection*{Some Notation}
From here on in, we may use bold face to denote vectors, e.g. ${\bf v}$ instead of $\vec{v}$. We also might use dots over quantities to represent time derivatives, e.g. $\dot{\bf v}=d{\bf v}/dt$. As mentioned above, repeated indices infer sums, e.g.,
\begin{eqnarray}
x_iy_i&=&\sum_i x_iy_i=\vec{x}\cdot\vec{y},\\
\nonumber
x_iA_{ij}y_j&=&\sum_{ij}x_iA_{ij}y_j,
\end{eqnarray}


\subsection{Exercises}

\begin{enumerate}

\item All physicists must become comfortable with thinking of oscillatory and wave mechanics in terms of expressions that include the form $e^{i\omega t}$.
\begin{enumerate}
\item Perform Taylor expansions in powers of $\omega t$ of the functions $\cos\omega t$ and $\sin\omega t$.
\item Perform a Taylor expansion of $e^{i\omega t}$.
\item Using parts (a) and (b) show that $e^{i\omega t}=\cos\omega t+i\sin\omega t$.
\item Show that $\ln(-1)=i\pi$.
\end{enumerate}

\item One of the many uses of the scalar product is to find the angle between two given vectors. Find the angle between the vectors $\vec{b}=(1,2,4)$ and $\vec{c}=(4,2,1)$ by evaluating their scalar product.

\item Use the product rule to show that
\[
\frac{d}{dt}(\vec{r}\cdot\vec{s})=\frac{d\vec{r}}{dt}\cdot\vec{s}+\vec{r}\cdot\frac{d\vec{s}}{dt}.
\]

\item Multiply the rotation matrix in Example \ref{ex:rotmat} by its transpose to show that the matrix is unitary or orthogonal, i.e. you get the unit matrix.

\item Find the matrix for rotating a coordinate system by 90 degrees about the $x$ axis.

\begin{comment}

\item Consider a rotation where in the new coordinate system the three axes are at angles $\alpha$, $\beta$ and $\gamma$ relative to the original $x$ axis. Show that
\[
\cos^2\alpha+\cos^2\beta+\cos^2\gamma=1.
\]

\item Find the rotation matrix for each of the three rotations, then find the matrix for the combined rotations.
First rotate by 90$^\circ$ around the $y$ axis to get to the primed system. Then rotate the primed system 90$^\circ$ about the new $z$ axis to get to the doubly primed system. Then find the matrix that rotates the doubly primed coordinate system 90$^\circ$ about the new $x$ axis to get to the triply primed system. Finally, find the matrix that performs all three operations sequentially.

\end{comment}

\item Consider a parity transformation which reflects about the $x=0$ plane. Find the matrix that performs the transformation. Find the matrix that performs the inverse transformation.

\item Show that the scalar product of two vectors is unchanged if both undergo the same rotation. Use the fact that the rotation matrix is unitary, $U_{ab}=U^{-1}_{ba}$.

\item Show that the product of two unitary matrices is a unitary matrix. 

\item Show that
\[
\sum_k\epsilon_{ijk}\epsilon_{klm}=\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}.
\]

\item Consider a cubic volume $V=L^3$ defined by $0<x<L$, $0<y<L$ and $0<z<L$. Consider a vector $\vec{A}$ that depends arbitrarily on $x,y,z$. Show how Gauss's law,
\[
\int_V dv\nabla\cdot\vec{A}=\int_Sd\vec{S}\cdot\vec{A},
\]
is satisfied by direct integration. I.e., you should use the fact that $\int_a^b dx~(d/dx)f(x)=f(b)-f(a)$.

\item Consider the function $z=3x^2-4y^2+12xy-6x+24$. Find any maxima or minima and determine whether it is a maximum or a minimum or an inflection point.

\item A real $n-$dimensional symmetric matrix $\lambda$ can always be diagonalized by a unitary transformation, i.e. there exists some unitary matrix $U$ such that,
\begin{eqnarray}
U_{ij}\lambda_{jk}U^{-1}_{km}&=&\tilde{\lambda}_{im}=\left(\begin{array}{cccc}
\tilde{\lambda}_{11}&0&\cdots&0\\
0&\tilde{\lambda}_{22}&\cdots&0\\
\vdots& & \ddots & \vdots\\
0& \cdots &\cdots &\tilde{\lambda}_{nn}
\end{array}\right).
\end{eqnarray}
The values $\tilde{\lambda}_{ii}$ are referred to as eigenvalues. The set of $n$ eigenvalues are unique, but their ordering is not -- there exists a unitary transformation that permutes the indices. 

Consider a function $f(x_1,\cdots,x_n)$ that has the property,
\begin{eqnarray}
\left.\partial_i f(\vec{x})\right|_{\vec{x}=0}=0,
\end{eqnarray}
for all $i$. Show that if this function is a minimum, and not a maximum or an inflection point, that the $n$ eigenvalues of the matrix
\begin{eqnarray}
\lambda_{ij}&\equiv&\left.\partial_i\partial_j f(\vec{x})\right|_{\vec{x}=0},
\end{eqnarray}
must be positive.

\item
\begin{enumerate}
\item For the unitary matrix $U$ that diagonalizes $\lambda$ as shown in the previous problem. Show that each row of the unitary matrix represents an orthogonal unit vector by using the definition of a unitary matrix.
\item Show that the vector
\begin{eqnarray}
x_i^{(k)}&\equiv& U_{ki}=(U_{k1},U_{k2},\cdots,U_{kn}),
\end{eqnarray}
has the property that
\begin{eqnarray}
\lambda_{ij}x^{(k)}_j&=&\tilde{\lambda}_{kk}x^{(k)}_i.
\end{eqnarray}
These vectors are known as eigenvectors, as they have the property that when multiplied by $\lambda$ the resulting vector is proportional ( same direction) as the original vector. Because one can transform to a basis, using $U$, where $\lambda$ is diagonalized, in the new basis the eigenvectors are simply the unit vectors.
\end{enumerate}
\end{enumerate} 


Our study begins with the study of the motion of bodies. Motion of a body means
that its position in space varies in time. The notion of motion is relative: a passenger
in a plane sitting in his chair has a fixed position relative to the plane, but moves at,
say 800 km/h relative to a person standing on earth. The latter moves at 800 km/h
relative to the passenger, in the opposite direction. To describe the motion we then
need a reference frame.
We normally live standing on earth and such are the laboratories in which we do
our experiments. Let us then start by choosing a reference frame fixed on the earth.
The possible choices are still infinite.
The position of a body is defined when we know were it is. The simplest case is
when we deal with a particle, a body that is so small that it can be considered
point-like. It is called a material point. Let us see how we can define the position of
a material point. For an extended body the positions of all its points should be
similarly defined.
To know the position of a point in space we need three numbers, one for each of
its dimensions. To define its position on a given surface, two numbers are needed

 compulsory, they are just generally the
 most convenient). The position of P is given by its two co-ordinates, which is an
 ordered pair of real numbers (x, y).
 Consider now a point in space. The reference frame shown in Fig. 1.1c is called
 a Cartesian rectangular right-handed frame, after René Descartes (1596–1650). It
 is made of three co-ordinate axes, called x, y and z. They cross in a single point, the
 origin of the frame. All the angles between the (three) pairs of axes are right. The
 length units on the three axes are equal. Finally we must choose positive orientations
 of the axes. There are two basic possibilities. Let us assume that we have
 already defined the positive directions of x and y. We have two possible choices for
 the positive direction of z. Figure 1.1c shows one of them; an observer standing
 with his feet on the xy plane lying along the z axis and looking down, willing to
 move the x axis on the y axis by a 90° rotation, sees this rotation happening
 anticlockwise. The second possibility is the opposite sign of z. The two frames are
 called right-handed and left-handed respectively.
 Now consider the inversion of the axes. If we start from a right-handed frame
 and invert one axis, that is a mirror reflection and we get a left-ended frame. The
 same happens if we invert all three axes. The inversion of two axes gives, on the~

